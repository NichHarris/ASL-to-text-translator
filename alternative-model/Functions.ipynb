{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544efd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for label in os.listdir('datasetzoo'):\n",
    "  label_path = os.path.join('datasetzoo', label)\n",
    "  for tensor_file in os.listdir(label_path):\n",
    "    file_path = os.path.join(label_path, tensor_file)\n",
    "    tensor = torch.load(file_path)\n",
    "    X.append(tensor)\n",
    "    Y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "unique_labels = list(set(labels))\n",
    "\n",
    "for tensor_file in tensor_files:\n",
    "  tensor = torch.load(tensor_file)\n",
    "  X.append(tensor)\n",
    "  label = os.path.dirname(tensor_file)\n",
    "  one_hot = np.zeros(len(unique_labels))\n",
    "  one_hot[unique_labels.index(label)] = 1\n",
    "  Y.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb35607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "unique_labels = os.listdir('datasetzoo')\n",
    "\n",
    "for label in unique_labels:\n",
    "  label_path = os.path.join('datasetzoo', label)\n",
    "  for tensor_file in os.listdir(label_path):\n",
    "    file_path = os.path.join(label_path, tensor_file)\n",
    "    tensor = torch.load(file_path)\n",
    "    X.append(tensor)\n",
    "    # One-hot encode \n",
    "    one_hot = np.zeros(len(unique_labels))\n",
    "    one_hot[unique_labels.index(label)] = 1\n",
    "    Y.append(one_hot)\n",
    "\n",
    "# Convert Y from a list of one-hot encoded labels to a numpy array\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e16f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "base_directory = 'signs/'\n",
    "\n",
    "# Lists to store the tensors and labels\n",
    "tensors = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all the class directories\n",
    "for class_name in os.listdir(base_directory):\n",
    "  class_directory = os.path.join(base_directory, class_name)\n",
    "  \n",
    "  for filename in os.listdir(class_directory):\n",
    "    # Load the tensor from the file\n",
    "    tensor = torch.load(os.path.join(class_directory, filename))\n",
    "    \n",
    "    # Add the tensor and label to the lists\n",
    "    tensors.append(tensor)\n",
    "    labels.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_paths = glob.glob('datasetz/*.pt')\n",
    "\n",
    "tensors = []\n",
    "\n",
    "class_names = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    class_name = file_path.split('_')[0]\n",
    "    \n",
    "    tensor_list = torch.load(file_path)\n",
    "    \n",
    "    # add the tensors and class names to the respective lists\n",
    "    tensors.extend(tensor_list)\n",
    "    class_names.extend([class_name] * len(tensor_list))\n",
    "\n",
    "# split the tensors into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tensors, class_names, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "X = []\n",
    "\n",
    "for label in labels:\n",
    "  samples = []\n",
    "  for tensor_file in os.listdir(label):\n",
    "    tensor = torch.load(os.path.join(label, tensor_file))\n",
    "    samples.append(tensor)\n",
    "  X.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test tensor size\n",
    "import torch\n",
    "\n",
    "# Load the tensor from the file\n",
    "tensor = torch.load('datasetz/bye_aug_1.pt')\n",
    "\n",
    "# Check the shape of the tensor\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f539b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.identity = nn.Identity()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.identity(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Print loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a104425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issues with tensorboard, give up for now\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_size, 64, activation=nn.ReLU())\n",
    "        self.fc2 = nn.Linear(64, 32, activation=nn.ReLU())\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Define model, loss function, and optimizer\n",
    "model = LSTMModel(258, signs.shape[0], 64)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define a tensorboard logger\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_logger = SummaryWriter(log_dir)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # ...\n",
    "    # Calculate loss and accuracy\n",
    "    # ...\n",
    "    tb_logger.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    tb_logger.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum viable model so far\n",
    "#Mistake was not converting to float32\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_rnn_layers, batch_first=True) #bidirectional=True, dropout= ???\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size, fc_hidden_size)\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, fc_hidden_size)\n",
    "        self.fc3 = nn.Linear(fc_hidden_size, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define initial tensors for hidden and cell states\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_lstm_layers, batch_size, self.lstm_hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_lstm_layers, batch_size, self.lstm_hidden_size).to(device) \n",
    "\n",
    "        # Pass input with initial tensors to lstm layers\n",
    "        outlstm,  = self.lstm(x, (h0, c0))\n",
    "        out_relu = self.relu(out_lstm)\n",
    "\n",
    "        # Pass last timestep to fc layers\n",
    "        in_fc1 = out_relu[:, -1, :]\n",
    "        out_fc1 = self.fc1(in_fc1)\n",
    "        in_fc2 = self.relu(out_fc1)\n",
    "\n",
    "        out_fc2 = self.fc2(in_fc2)\n",
    "        in_fc3 = self.relu(out_fc1)\n",
    "\n",
    "        out_fc3 = self.fc3(in_fc3)\n",
    "        out = self.softmax(out_fc3)\n",
    "\n",
    "        return out\n",
    "\n",
    "num_rnn_layers = 3\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Split the dataset with sci-kit learn before this, categorical split\n",
    "X_train = X_train.to(torch.float32)\n",
    "y_train = y_train.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplify\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.identity = nn.Identity()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.identity(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Print loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Loop through the training data for a specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through the individual training examples\n",
    "    for inputs, labels in training_data:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: pass the inputs through the model to get the predicted output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute the gradients of the loss with respect to the model's parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.identity = nn.Identity()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.identity(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Loss and accuracy\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == y_train).float().mean()\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item(), 'Accuracy: ', accuracy.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum viable model so far\n",
    "#Add relu to it\n",
    "#Mistake was not converting to float32\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.identity = nn.Identity()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.identity(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Loss and accuracy\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == y_train).float().mean()\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item()*100, '%', 'Accuracy: ', accuracy.item()*100, '%')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.identity = nn.Identity()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.identity(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.identity(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Create a new tensor for X_train that shares the same memory and does not require gradient calculations\n",
    "X_train = X_train.clone().detach()\n",
    "\n",
    "# Convert X_train to a tensor with dtype torch.float32\n",
    "X_train = X_train.type(torch.float32)\n",
    "\n",
    "# Create a new tensor for y_train that shares the same memory and does not require gradient calculations\n",
    "y_train = y_train.clone().detach()\n",
    "\n",
    "# Convert y_train to a tensor with dtype torch.long\n",
    "y_train = y_train.type(torch.long)\n",
    "\n",
    "# Reshape X_train to have a shape of (batch_size, sequence_length, input_dim)\n",
    "X_train = X_train.reshape(-1, 30, 258)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == y_train).float().mean()\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    print(f'Epoch: {epoch+1:03d} | Loss: {loss:.5f} | Accuracy: {accuracy:.5f}')\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "model = torch.load('model.pth')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tb.SummaryWriter()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Train the model on one epoch\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        # Write the loss to TensorBoard\n",
    "        writer.add_scalar('val_loss', loss, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "X_test = X_test.clone().detach()\n",
    "y_test = y_test.clone().detach()\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(X_test)\n",
    "\n",
    "# Compute loss and accuracy\n",
    "test_loss = loss_fn(y_pred, y_test)\n",
    "_, prediction = y_pred.max(dim=1)\n",
    "test_accuracy = (prediction == y_test).float().mean()\n",
    "\n",
    "print('Test loss: ', test_loss.item()*100, '%', 'Test accuracy: ', test_accuracy.item()*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0244d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def optimize(trial):\n",
    "    input_dim = 258\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    output_dim = signs.shape[0]\n",
    "\n",
    "    model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=trial.suggest_loguniform('lr', 1e-5, 1e-1))\n",
    "  \n",
    "    for epoch in range(500):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        _, prediction = y_pred.max(dim=1)\n",
    "        accuracy = (prediction == y_train).float().mean()\n",
    "    \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if trial.should_prune(epoch, loss.item()):\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return accuracy.item()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print('Best params: ', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Reshape X_train to have a shape of (batch_size, sequence_length, input_dim)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 30, 258)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Compute model predictions on X_train\n",
    "with torch.no_grad():\n",
    "    yhat = model(X_train)\n",
    "\n",
    "# Convert predictions and true labels to NumPy arrays\n",
    "yhat = yhat.numpy()\n",
    "ytrue = ytrue.numpy()\n",
    "\n",
    "# Compute confusion matrix and accuracy score\n",
    "confusion_matrix = multilabel_confusion_matrix(ytrue, yhat)\n",
    "accuracy = accuracy_score(ytrue, yhat)\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Print model summary\n",
    "summary(model, X_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Build the confusion matrix\n",
    "cm = confusion_matrix(y_train, ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_data(X_test)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "    yhat = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model \n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "X_test = np.stack([np.array(t, dtype=np.float32) for t in X_test], axis=0)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    loss = loss_fn(y_pred, y_test)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == y_test).float().mean()\n",
    "    print('Test loss: ', loss.item(), 'Test accuracy: ', accuracy.item()*100, '%')\n",
    "    \n",
    "    # Calculate predicted class labels\n",
    "    prediction_labels = [label_map[i] for i in prediction.argmax(dim=1)]\n",
    "    \n",
    "    # Calculate true class labels\n",
    "    true_labels = [label_map[i] for i in y_test.argmax(dim=1)]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confusion_matrix = pd.crosstab(true_labels, prediction_labels, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert the test set labels to integers\n",
    "y_test_int = y_test_np.argmax(axis=1)\n",
    "\n",
    "# Convert the predictions to integers\n",
    "predictions_int = predictions_np.argmax(axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_int, predictions_int)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the prediction and true labels to tensors\n",
    "prediction = torch.tensor(prediction, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = torch.zeros(len(labels), len(labels))\n",
    "for t, p in zip(y_test, prediction):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "# Convert the confusion matrix to a Pandas dataframe for easier visualization\n",
    "cm_df = pd.DataFrame(cm.numpy(), index=labels, columns=labels)\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(cm_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import mediapipe.python.solutions.holistic as mp_holistic\n",
    "\n",
    "folder = os.path.join('MP_Data') \n",
    "\n",
    "signs = np.array(['hello', 'sorry', 'help'])\n",
    "\n",
    "sign_names = signs.tolist()\n",
    "\n",
    "\n",
    "# Initialize the camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "  # Capture a frame from the camera\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  # Preprocess the frame to extract the features\n",
    "  features = preprocess_frame(frame)\n",
    "\n",
    "  # Convert the features to a tensor and pass them through the model\n",
    "  features = torch.tensor(features, dtype=torch.float32)\n",
    "  prediction = model(features)\n",
    "  \n",
    "  # Get the index of the most likely action\n",
    "  _, index = prediction.max(dim=1)\n",
    "  index = index.item()\n",
    "  \n",
    "  # Map the index to the corresponding action name\n",
    "  action_name = sign_names[index]\n",
    "  \n",
    "  # Display the action name on the frame\n",
    "  cv2.putText(frame, action_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "  \n",
    "  # Show the frame\n",
    "  cv2.imshow('Frame', frame)\n",
    "  \n",
    "  # Break out of the loop if the user presses 'q'\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "folder = os.path.join('MP_Data') \n",
    "\n",
    "signs = np.array(['hello', 'sorry', 'help'])\n",
    "\n",
    "sign_names = signs.tolist()\n",
    "\n",
    "# Initialize the camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "  # Capture a frame from the camera\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  # Preprocess the frame to extract the features\n",
    "  features = preprocess_frame(frame)\n",
    "\n",
    "  # Convert the features to a tensor and pass them through the model\n",
    "  features = torch.tensor(features, dtype=torch.float32)\n",
    "  prediction = model(features)\n",
    "  \n",
    "  # Get the index of the most likely action\n",
    "  _, index = prediction.max(dim=1)\n",
    "  index = index.item()\n",
    "  \n",
    "  # Map the index to the corresponding action name\n",
    "  action_name = sign_names[index]\n",
    "  \n",
    "  # Display the action name on the frame\n",
    "  cv2.putText(frame, action_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "  \n",
    "  # Show the frame\n",
    "  cv2.imshow('Frame', frame)\n",
    "  \n",
    "  # Break out of the loop if the user presses 'q'\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Set up the camera\n",
    "camera = cv2.VideoCapture(0)  # 0 indicates that you want to use the default camera\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    _, frame = camera.read()\n",
    "\n",
    "    # Pre-process the frame\n",
    "    keypoints = extract_keypoints(frame)  # Extract keypoints/landmarks from the frame\n",
    "    keypoints = keypoints.to(torch.float32)  # Convert keypoints to a PyTorch tensor\n",
    "    keypoints = keypoints.unsqueeze(0)  # Add a batch dimension to the keypoints tensor\n",
    "\n",
    "    # Pass the keypoints through the model\n",
    "    output = model(keypoints)\n",
    "\n",
    "    # Get the index of the class with the highest probability\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "\n",
    "    # Draw the prediction on the frame\n",
    "    cv2.putText(frame, str(pred.item()), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if key == ord('q'):  # If the user pressed 'q', break out of the loop\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy the window\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a66f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
