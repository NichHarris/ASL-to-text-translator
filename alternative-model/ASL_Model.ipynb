{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np # data set will be in numpy array format\n",
    "import os # for file paths\n",
    "from matplotlib import pyplot as plt\n",
    "import time # delay between each frame\n",
    "import mediapipe as mp # holistic \n",
    "mp_holistic = mp.solutions.holistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "folder_path = 'MVP_dataset'\n",
    "signs = []\n",
    "unique_signs = set()\n",
    "\n",
    "if os.listdir(folder_path):\n",
    "    for sign in os.listdir(folder_path):\n",
    "        sign_path = os.path.join(folder_path, sign)\n",
    "        unique_signs.add(sign)\n",
    "        for video in os.listdir(sign_path):\n",
    "            video_path = os.path.join(sign_path, video)\n",
    "            video_keypoints = []\n",
    "            for file in os.listdir(video_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    file_path = os.path.join(video_path, file)\n",
    "                    keypoints = np.load(file_path)\n",
    "                    video_keypoints.append(keypoints)\n",
    "            signs.append(video_keypoints)\n",
    "\n",
    "    signs = np.array(signs)\n",
    "\n",
    "    if signs.size:\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(signs)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        signs_labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "    else:\n",
    "        print(\"The signs array is empty.\")\n",
    "else:\n",
    "    print(\"No data to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcaa617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs_labels.shape[1])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Split the dataset with sci-kit learn before this, categorical split\n",
    "signs = torch.tensor(signs, dtype=torch.float32)\n",
    "signs_labels = torch.tensor(signs_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Forward pass\n",
    "    y_pred = model(signs)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    loss = loss_fn(y_pred, signs_labels)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == signs_labels).float().mean()\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item(), 'Accuracy: ', accuracy.item()*100, '%')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'mvpX.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab55d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loading a model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "signs = np.array(['hello', 'sorry', 'help'])\n",
    "model = LSTMModel(input_dim=258, hidden_dim=64, output_dim=signs.shape[0])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model = torch.load('mvp1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def optimize(trial):\n",
    "    input_dim = 258\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    output_dim = actions.shape[0]\n",
    "\n",
    "    model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=trial.suggest_loguniform('lr', 1e-5, 1e-1))\n",
    "  \n",
    "    for epoch in range(500):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        _, prediction = y_pred.max(dim=1)\n",
    "        accuracy = (prediction == y_train).float().mean()\n",
    "    \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if trial.should_prune(epoch, loss.item()):\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return accuracy.item()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print('Best params: ', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model \n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(signs)\n",
    "    loss = loss_fn(y_pred, signs_labels)\n",
    "    _, prediction = y_pred.max(dim=1)\n",
    "    accuracy = (prediction == signs_labels).float().mean()\n",
    "    print('Test loss: ', loss.item(), 'Test accuracy: ', accuracy.item()*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Find the class with the highest probability for each sample\n",
    "_, predictions_np = y_pred.max(dim=1)\n",
    "\n",
    "# Convert the predictions to NumPy arrays\n",
    "predictions_np = predictions_np.numpy()\n",
    "\n",
    "# Convert the test set labels to integers\n",
    "y_test_int = y_test.argmax(axis=1)\n",
    "\n",
    "# Convert the predictions to integers\n",
    "predictions_int = predictions_np.argmax(axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_int, predictions_int)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First part from Nick\n",
    "def preprocess_frame(frame):\n",
    "    \n",
    "    frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame)  \n",
    "\n",
    "    left_hand, right_hand, pose, landmarks = np.zeros(21 * 3), np.zeros(21 * 3), np.zeros(33 * 4), np.zeros(258)\n",
    "\n",
    "    if not results.left_hand_landmarks:\n",
    "        left_hand = np.zeros(21 * 3)\n",
    "    else:\n",
    "        lh = results.left_hand_landmarks\n",
    "        for i, landmark in enumerate(lh.landmark):\n",
    "            shift_ind = i * 3\n",
    "            left_hand[shift_ind] = landmark.x\n",
    "            left_hand[shift_ind + 1] = landmark.y\n",
    "            left_hand[shift_ind + 2] = landmark.z            \n",
    "\n",
    "    if not results.right_hand_landmarks:\n",
    "        right_hand = torch.zeros(21 * 3)\n",
    "    else:\n",
    "        rh = results.right_hand_landmarks\n",
    "        for j, landmark in enumerate(rh.landmark):\n",
    "            shift_ind = j * 3\n",
    "            right_hand[shift_ind] = landmark.x\n",
    "            right_hand[shift_ind + 1] = landmark.y\n",
    "            right_hand[shift_ind + 2] = landmark.z\n",
    "\n",
    "    if not results.pose_landmarks:\n",
    "        pose = torch.zeros(33 * 4)\n",
    "    else:\n",
    "        # Add pose keypoints (25 w/ 3d coordinates plus visbility probability)\n",
    "        pl = results.pose_landmarks\n",
    "        for k, landmark in enumerate(pl.landmark):\n",
    "            # Ignore lower body (landmarks #25-33)\n",
    "\n",
    "\n",
    "            shift_ind = k * 4\n",
    "            pose[shift_ind] = landmark.x\n",
    "            pose[shift_ind + 1] = landmark.y\n",
    "            pose[shift_ind + 2] = landmark.z  \n",
    "            pose[shift_ind + 3] = landmark.visibility  \n",
    "\n",
    "    # Concatenate processed frame\n",
    "    landmarks = np.concatenate([pose, left_hand, right_hand])\n",
    "    \n",
    "\n",
    "    hand_detected = False\n",
    "    # Check if left hand is in frame\n",
    "    if left_hand.any() and (np.min(left_hand[:21*3:3]) > x_min) and (np.max(left_hand[:21*3:3]) < x_max) and (np.min(left_hand[1:21*3:3]) > y_min) and (np.max(left_hand[1:21*3:3]) < y_max):\n",
    "        hand_detected = True\n",
    "    # Check if right hand is in frame\n",
    "    if right_hand.any() and (np.min(right_hand[:21*3:3]) > x_min) and (np.max(right_hand[:21*3:3]) < x_max) and (np.min(right_hand[1:21*3:3]) > y_min) and (np.max(right_hand[1:21*3:3]) < y_max):\n",
    "        hand_detected = True\n",
    "\n",
    "    return landmarks, hand_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43229abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holistic_model():\n",
    "    # Get Mediapipe holistic solution\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "\n",
    "    # Instantiate holistic model, specifying minimum detection and tracking confidence levels\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        static_image_mode=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) \n",
    "    \n",
    "    return holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b847748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "accuracy = 0.4\n",
    "x_min, x_max = 0, 100\n",
    "y_min, y_max = 0, 100\n",
    "counter = 0\n",
    "prev_pred = -1\n",
    "current_prediction = \"\"\n",
    "holistic = get_holistic_model()\n",
    "queue = deque(maxlen=48)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        landmarks, hand_detected = preprocess_frame(frame)\n",
    "        \n",
    "        if hand_detected:  \n",
    "                queue.append(landmarks)\n",
    "                queue_model = torch.tensor(queue, dtype=torch.float32)\n",
    "                output = model(queue_model)\n",
    "                res = output[0]\n",
    "                res = res.detach().numpy()\n",
    "                \n",
    "                if len(queue) == 48 and np.max(res) > accuracy:\n",
    "                    pred = np.argmax(res)\n",
    "                    if prev_pred == pred:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "                        prev_pred = pred\n",
    "                    if counter == 5:\n",
    "                        current_prediction = signs[pred]\n",
    "                        prediction = signs[pred]\n",
    "                        cv2.putText(frame, f'Prediction: {prediction}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f'Prediction: {current_prediction}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        # Show the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "\n",
    "        # Check for user input\n",
    "        key = cv2.waitKey(1)  # Wait for 1ms for the user to press a key\n",
    "        if key == ord('q'):  # If the user pressed 'q', break out of the loop\n",
    "            break\n",
    "\n",
    "# Release the camera and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323a4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
